\documentclass[bachelor, zaoch, coursework]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}


% Custom colors
\usepackage{color}
\usepackage{xcolor}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

\usepackage[english,russian]{babel}


\usepackage[colorlinks=true]{hyperref}


% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Деревья решений. Алгоритм C4.5}

% Курс
\course{2}

% Группа
\group{251}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Ахмановой Элины Дамировны}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{к.\,ф.-м.\,н.} %должность, степень, звание
\saname{С.\,В.\,Миронов}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
%\patitle{к.\,ф.-м.\,н., доцент}
%\paname{Д.\,Ю.\,Петров}

% Семестр (только для практики, для остальных
% типов работ не используется)
%\term{2}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{2}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
%%\practFinish{14.07.2016}

% Год выполнения отчета
\date{2018}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro
Выполненная курсовая работа посвящена решающим деревьям и алгоритму C4.5. Решающие деревья - это семейство моделей, позволяющих восстанавливать нелинейные зависимости разной сложности \cite{course_mipth}. Эти деревья относят к классу логических методов \cite{quinlan_c45}. Основной идеей можно считать объединение определенного количества более простых решающих правил. Именно благодаря этому реализации можно считать интерпретируемым. 


В одной из наиболее действенных реализаций, алгоритме C4.5, для достижения максимальной обучаемости используется способ объединения нескольких вершин в одну. При этом должен использоваться критерий целесообразности объединения двух вершин.

Целью данной работы является реализация алгоритма C4.5 и изучение решающих деревьев, аналогов алгоритма C4.5. Для реализации используется язык программирования Python, так как именно он является одним из наиболее оптимальных средств решения математических задач.

Первые работы по использованию решающих деревьев берут начало с 60-х годов прошлого века. Алгоритм C4.5 разработан Джоном Квинланом \cite{quinlan_learning} как модификация другого алгоритма того же автора. Актуальность предоставленной работы заключается в большом количестве составных задач классификации или регрессии, для которых алгоритм может быть использован в составе композиций - решающих лесов. 


%%
%ГЛАВА 1
%%
\section{Решающие деревья}

% 1.1 "Определение"
\subsection{Определение и построение решающих деревьев}
	Работа посвящена важному классу методов машинного обучения, решающим деревьям. Эти методы созданы для решения задач классификации (когда предсказываемый результат является классом), а в специальных реализациях - и для задач регрессии (когда предсказываемый результат можно рассматривать как вещественное число: цена на дом, продолжительность пребывания пациента в больнице). 
	
Особенность решающих деревьев заключается в том, что они решают задачу получения коэффициента важности всех используемых признаков. 
	
	Область применения деревья решений в настоящее время широка, но все задачи, решаемые этим аппаратом могут быть объединены в следующие три класса \cite{solov_intellect}:
\begin{itemize}
	\item Описание данных: Деревья решений позволяют хранить информацию о данных в компактной форме, вместо них мы можем хранить дерево решений, которое содержит точное описание объектов.
	\item Классификация: Деревья решений отлично справляются с задачами классификации, т.е. отнесения объектов к одному из заранее известных классов. Целевая переменная должна иметь дискретные значения.
	\item Регрессия: Если целевая переменная имеет непрерывные значения, деревья решений позволяют установить зависимость целевой переменной от независимых(входных) переменных. Например, к этому классу относятся задачи численного прогнозирования (предсказания значений целевой переменной).
\end{itemize}
	
	Решающие деревья создавались с целью попытки формализовать способ мышления, который обычно люди используют для принятия решений. Это хорошо описывает логику принятия решений банком для выдачи кредита: 

\begin{itemize}
	\item возраст клиента;
	\item заработная плата; 
	\item рабочий стаж;
	\item наличие других кредитов и кредитная история.
\end{itemize}

Также это отлично иллюстрирует процесс работы врача, когда тот говорит с пациентом, задает ему один за другим уточняющие вопросы, может дать необходимые советы. 

Дерево решений \cite{lecture_ul} – это способ представления правил в иерархической, последовательной структуре  \cite{course_intuit}. Основа такой структуры – ответы “Да” или “Нет” в случае бинарных деревьев (алгоритм CART \cite{breiman_class}), или на ряд вопросов, определимых динамически в случае деревьев с произвольным числом потомков (алгоритм C4-5 \cite{quinlan_c45}).

Каждой вершине дерева за исключением конечной листовой соответствует вопрос с несколькими вариантами ответа. Ответы будут являться выходящими ребрами. В зависимости от ответа осуществляется переход к одной из нижних вершин. Концевая вершина, лист, соответствует одному из классов. При положительном ответе на вопрос осуществляется переход к левой части дерева, называемой левой ветвью, при отрицательном – к правой части дерева. Таким образом, внутренний узел дерева является узлом проверки определенного условия.Для каждой конкретной задачи существуют свои типы конечного узла в количестве $n+1$.

Решающие деревья являются ациклическими графами и воспроизводят логические схемы, позволяющие получить окончательное решение о классификации объекта с помощью ответов на  иерархически организованную систему вопросов \cite{senko_course}. 

Обратим внимание на то, что заданный на последнем иерархическом уровне вопрос зависит от полученного на предыдущем уровне ответа. Подобные модели свойственны для использования в:

 \begin{itemize}
	\item медицине;
	\item ботанике; 
	\item  зоологии;
	\item  социологии;
	\item экономике.
\end{itemize}

 Решающее дерево называется бинарным, если каждая внутренняя или корневая вершина поставлена в соответствие только двум выходящим рёбрам. Бинарные деревья удобно использовать в моделях машинного обучения \cite{senko_course}. 


Модель, представленная в виде дерева решений, является интуитивной и упрощает понимание решаемой задачи. Результат работы алгоритмов конструирования деревьев решений, в отличие, например, от нейронных сетей, представляющих собой “черные ящики”, легко интерпретируется пользователем. Это свойство деревьев решений не только важно при отнесении к определенному классу нового объекта, но и полезно при интерпретации модели классификации в целом \cite{course_intuit}.

Дерево решений позволяет понять и объяснить, почему конкретный объект относится к тому или иному классу. Деревья решений дают возможность извлекать правила из базы данных на естественном языке \cite{course_intuit}.
 
 
 
 \textbf{Распознавание с помощью решающих деревьев}    Предположим, что бинарное дерево T используется для распознавания объектов, описываемых набором признаков $X_1, X_2, . . , X_n$ \cite{senko_course}.
     
Каждой вершине $\nu$ дерева $T$ ставится в соответствие предикат, касающийся значения одного из признаков. Непрерывному признаку $X_j$ соответствует предикат вида "$X_j \geqslant \delta^x_j$", где $j$ - некоторый пороговый параметр ~\ref{pril:7}..

Категориальному признаку $X_{j^1}$ принимающему значения из множества $M_{j^1} = { a^{j^1}_1, . . , a^{i^1}_{r(j^1)}}$ ставится в соответствие предикат вида "$X_{j^1} \in M_{j^1}^{v1}$", где $M_{j^1}^{v1}$ является элементом дихотомического разбиения $M_{j^1}^{v1}, M_{j^1}^{v2}$ множества $M_{j^1}$. Выбор одного из двух, выходящих из вершины $\nu$ ребер производится в зависимости от значения предиката.

Процесс распознавания заканчивается при достижении концевой вершины (листа). Объект относится классу согласно метке, поставленной в соответствие данному листу. 

Процесс распознавания заканчивается при достижении концевой вершины (листа). Объект относится классу согласно метке, поставленной в соответствие данному листу. 

\textbf{ Обучение решающих деревьев } Рассмотрим задачу распознавания с классами $K_1, . . , K_L$. Обучение производится по обучающей выборке $\widetilde{S_t}$ и включает в себя поиск оптимальных пороговых параметров или оптимальных дихотомических разбиений для признаков $X_1, . . , X_n $ \cite{sirota_method}. 

При этом поиск производится исходя из требования снижения среднего индекса неоднородности в выборках, порождаемых искомым дихотомическим разбиением обучающей выборки  $\widetilde{S_t}$ .

Индекс неоднородности вычисляется для произвольной выборки $\widetilde{S}$, содержащей объекты из классов $K_1, . . , K_L$. При этом используется несколько видов индексов, включая: 

 \begin{itemize}
	\item энтропийный индекс неоднородности;
	\item индекс Джини \cite{sen_16}; 
	\item  индекс ошибочной классификации.
\end{itemize}

Энтропийный индекс неоднородности вычисляется по формуле \cite{shannon_math}:

\begin{equation}
	\gamma_e(\widetilde{S}) = - \sum{P_i \ln{P_i}},
\end{equation}

где $P_i$ - доля объектов класса $K_i$ в выборке  $\widetilde{S}$. При этом принимается, что $0 \ln{(0)} = 0$. Наибольшее значение $\gamma_e(\widetilde{S})$ принимает при равенстве долей классов. Наименьшее значение $\gamma_e(\widetilde{S})$ достигается при принадлежности всех объектов одному классу. 

Индекс Джини вычисляется по формуле

\begin{equation}
	\gamma_e(\widetilde{S}) = - \sum{P_i ^2}.
\end{equation}

Индекс ошибочной классификации вычисляется по формуле

\begin{equation}
	\gamma_e(\widetilde{S}) = 1 - \max{(P_i)}.
\end{equation}

Нетрудно понять, что индексы (2) и (3) также достигают минимального значения при принадлежности всех объектов обучающей выборке одному классу. 

Предположим, что в методе обучения используется индекс неоднородности $\gamma_*$. Для оценки эффективности разбиения обучающей выборки $\widetilde{S_t}$ на непересекающиеся подвыборки $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$ используется уменьшение среднего индекса неоднородности в $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$ по отношению к $\widetilde{S_t}$.

Данное уменьшение вычисляется по формуле:

\begin{equation}
	\Delta(\gamma_*\widetilde{S_t}) = \gamma_*(\widetilde{S_t}) - P_l\gamma_*(\widetilde{S_t^l}) - P_r\gamma_*(\widetilde{S_t^r}).
\end{equation}

где $P_l$, $P_r$ являются долями $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$ в выборке $\widetilde{S_t}$. На первом этапе обучения бинарного решающего дерева ищется оптимальный предикат соответствующий корневой вершине. С этой целью оптимальные разбиения строятся для каждого из признаков из набора $X_1, . . , X_n $.  Выбирается признак $X_{i\max}$ с максимальным значением индекса $\Delta(\gamma_*\widetilde{S_t})$. Подвыборки $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$, задаваемые оптимальным предикатом для $X_{i\max}$ оцениваются с помощью критерия остановки. 

В качестве критерия остановки может быть использован простейший критерий достижения полной однородности по одному из классов. В случае, если какая-нибудь из выборок  $\widetilde{S_t ^*}$ удовлетворяет критерию остановки, то соответствующая вершина дерева объявляется концевой и для неё вычисляется метка класса. В случае, если выборка $\widetilde{S_t ^*}$  удовлетворяет критерию остановки, то формируется новая внутренняя вершина, для которой процесс построения дерева продолжается \cite{senko_course}. 
 

Однако вместо обучающей выборки $\widetilde{S_t}$ используется соответствующая вновь образованной внутренней вершине $\nu$ выборка $\widetilde{S_t}$, которая равна $\widetilde{S_t ^*}$. 

Для данной выборки производятся те же самые построения, которые на начальном этапе проводились для обучающей выборки $\widetilde{S_t}$ . 

Обучение может проводиться до тех пор, пока все вновь построенные вершины не окажутся однородными по классам. Такое дерево может быть построено всегда, когда обучающая выборка не содержит объектов с одним и тем же значениям каждого из признаков, принадлежащих разным классам. Однако абсолютная точность на обучающей выборке не всегда приводить к высокой обобщающей способности в результате эффекта переобучения. 

Одним из способов достижения более высокой обобщающей способности является использования критериев остановки, позволяющих остановит процесс построения дерева до того, как будет достигнута полная однородность концевых вершин. 

Рассмотрим несколько таких критериев \cite{bies_genetic}. 
    
\begin{enumerate}
	\item Критерий остановки по минимальному допустимому числу объектов в выборках, соответствующих концевым вершинам. 
	\item Критерий остановки по минимально допустимой величине индекса $\Delta(\gamma_*\widetilde{S_t})$ . Предположим, что некоторой вершине $\nu$ соответствует выборка $\widetilde{S_{\nu}}$, для которой найдены оптимальный признак вместе с оптимальным предикатом, задающим разбиение $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . Вершина $\nu $ считается внутренней, если индекc $\Delta(\gamma_*\widetilde{S_t})$ превысил пороговое значение $\tau$ и считается концевой в противном случае. 
	\item Критерий остановки по точности на контрольной выборке. Исходная выборка данных случайным образом разбивается на обучающую выборку $\widetilde{S_t}$ и контрольную выборку $\widetilde{S_c}$ . Выборка $\widetilde{S_t}$ используется для построения бинарного решающего дерева. Предположим, что некоторой вершине $\nu$ соответствует выборка $\widetilde{S_\nu}$ , для которой найдены оптимальный признак вместе с оптимальным предикатом, задающим разбиение  $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$.
	
	На контрольной выборке $\widetilde{S_c}$ производится сравнение эффективность распознающей способности деревьев $T_\nu$ и $T_nu^{++}$ . 

Деревья $T_\nu$ и $T_nu^{++}$ включают все вершины и рёбра, построенные до построения вершины $\nu$ . В дереве $T_\nu$ вершина $\nu$ считается концевой. В дереве $T_nu^{++}$ вершина $\nu$ считается внутренней, а концевыми считаются вершины, соответствующие подвыборкам $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . Распознающая способность деревьев $T_\nu$ и $T_nu^{++}$ сравнивается на контрольной выборке $\widetilde{S_c}$ . В том, случае если распознающая способность $T_nu^{++}$ превосходит распознающую способность $T_\nu$ все дальнейшие построения исходят из того, что вершина $\nu$ является концевой. В противном случае производится исследование $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ .

	\item Статистический критерий \cite{berger_statist}. Заранее фиксируется пороговый уровень значимости $(P<0.05, p<0.001$ или $p<0.001)$ . Предположим, что нам требуется оценить, является ли концевой вершина, для которой найдены оптимальный признак вместе с оптимальным предикатом, задающим разбиение $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . 
	
Исследуется статистическая достоверность различий между содержанием объектов распознаваемых классов в подвыборках $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . Для этих целей может быть использованы известные статистический критерий: Хи-квадрат и другие критерии. 

По выборкам $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ рассчитывается статистика критерия и устанавливается соответствующее p-значение. В том случае, если полученное p-значение оказывается меньше заранее фиксированного уровня значимости вершина $\nu$ считается внутренней. В противном случае вершина $\nu$ считается концевой. 
\end{enumerate}

Использование критериев ранней остановки не всегда позволяет адекватно оценить необходимую глубину дерева. Слишком ранняя остановка ветвления может привести к потере информативных предикатов, которые могут быть на самом деле найдены только при достаточно большой глубине ветвления. 

В связи с этим нередко целесообразным оказывается построение сначала полного дерева, которое затем уменьшается до оптимального с точки зрения достижения максимальной обучающей способности размера путём объединения некоторых концевых вершин. Такой процесс в литературе принято называть "pruning" ("подрезка") \cite{hastie_elements}. 

При подрезке дерева может быть использован критерий целесообразности объединения двух вершин, основанный на сравнение на контрольной выборке точности распознавания до и после проведения "подрезки".

Ещё один способ оптимизации обобщающей способности деревьев основан на учёте при "подрезке" дерева до некоторой внутренней вершины $\nu$ одновременно увеличения точности разделения классов на обучающей выборке и увеличения сложности, которые возникают благодаря ветвлению из $\nu$. 

% 1.1 "Определение"
\subsection{Алгоритмы деревьев решений}

\begin{itemize}
    \item  ID3 (Induction of Decision Tree \cite{quinlan_machine}) - рекурсивная процедура, которая принимает на вход некоторую подвыборку обучающей выборки. Когда она вызывается впервые, на вход подается вся выборка. Если все объекты выборки лежат в одном классе, образуется и возвращается новая листовая вершина, которая содержит этот класс. Иначе ищется предикат, который максимально хорошо выделяет часть классов. По этому предикату строятся две новые вершины.
    
    Этот алгоритм является наиболее старым и лаконичным способом использования решающих  деревьев. К плюсам данного алгоритма можно отнести: простоту, интерпретируемость, гибкость задания множества предикатов, возможность использования данных с пропусками, линейная трудоемкость. 
    
    Однако недостатки вытекают из того, что это жадная стратегия, и решения о ветвлении оптимальны локально. Кроме того, данная процедура очень чувствительна к составу выборки и шумовым признакам. 
    \item  C4.5 \cite{quinlan_c45} - улучшенная версия алгоритма ID3, где выбор атрибута происходит на основании нормализированного прироста информации, использует алгоритм усечения решающих деревьев и избавляет от проблемы переобучения, переусложнения структуры дерева, создавая контрольную выборку данных, которая обычно вдвое короче обучающей. Эта выборка используется для сокращения количества вершин в итоговом дереве. 
    \item CART (Classification and Regression Tree) - жадный рекурсивный алгоритм построения дерева решений, использующий стратегии усечения, обобщенный для решения задач регрессии.
    \item MARS - алгоритм, который использует расширение деревьев решений для улучшения обработки цифровых данных.
\end{itemize}

% 1.1 "Определение"
\subsection{Алгоритм C4.5}

C4.5 - алгоритм построения дерева решений, созданный Россом Куинланом \cite{quinlan_improved}. как модификация более простого алгоритма ID3. Этот алгоритм используется для задач классификации, поэтому его часто упоминают как статический классификатор. 

C4.5 строит деревья решений таким же образом, как это делает ID3, используя тестовую выборку и понятие информационной энтропии \cite{martin_math}.  

Информационная энтропия - это мера неопределенности некоторой ситуаци \cite{shennon_rabot}. Можно также назвать ее мерой рассеяния и в этом смысле она подобна дисперсии. Но если дисперсия является адекватной мерой рассеяния лишь для специальных распределений вероятностей случайных величин (а именно – для двухмоментных распределений, в частности, для гауссова распределения), то энтропия не зависит от типа распределения.

C4.5 моделирует дерево решений с неограниченным количеством ветвей у узла. Данный алгоритм может работать только с дискретным зависимым атрибутом и поэтому может решать только задачи классификации.

Для работы алгоритма C4.5 необходимо соблюдение следующих требований:

\begin{itemize}
    \item каждая запись набора данных должна быть ассоциирована с одним из предопределенных классов, т.е. один из атрибутов набора данных должен являться меткой класса;
    
    \item классы должны быть дискретными;
    
    \item каждый пример должен однозначно относиться к одному из классов;
    
    \item количество классов должно быть значительно меньше количества записей в исследуемом наборе данных. 
\end{itemize}

Простейшая реализация алгоритма аналогична ID3:

\begin{Verbatim}[fontsize=\small, numbers=left]
Input: database D
 Tree = {}
If D is pure OR other stopping criteria Then
	Terminate
End If
For all attribute a ? D do
	Entropy for root
End For

abest = Best attribute entropy
Tree = Create a node then test a best in the root
Dv = sud_datasets from D
For all Dv do
	Tree v= C4.5(Dv)
End for
Return Tree
\end{Verbatim}

% 1.1 "Оценка сложности"
\subsection{Оценка сложности алгоритма C4.5}

Алгоритм решающего дерева имеет сложность, линейную по длине выборки. Если множество предикатов настолько богато, что на шаге всегда находится предикат, разбивающий выборку $\widetilde{S_t}$ на непустые подмножества $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$,то алгоритм строит бинарное решающее дерево, безошибочно классифицирующее выборку $\widetilde{S_t}$. 

Общая схема построения дерева принятия решений выглядит следующим образом:

\begin{enumerate}
	\item Выбирается очередной атрибут, помещается в корень.
	
	\item Для всех его значений рекурсивно строится дерево в этом потомке. 
	
	Начиная со второго шага, алгоритм может быть распараллелен.

\end{enumerate}


\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{alg.png}
	\caption{\label{fig:alg}
	 Зависимость времени работы алгоритма от объема обучающей выборки}
\end{figure}

На рисунке ~\ref{fig:alg} изображена скорость работы трех методов:

\begin{enumerate}

	\item Линейные вычисления.
	\item  Распараллеливание в один поток (блокирование внешних потоков).
	\item Полностью распараллеленный алгоритм на 2 потока .
	
\end{enumerate}

Из графика ~\ref{fig:alg} видно, что распараллеливание алгоритма деревьев решений имеет смысл проводить только на больших объемах данных. Для случаев, где выборка данных меньше 50 элементов распараллеливание избыточно, линейный вариант алгоритма работает быстрее.


%%
%ГЛАВА 2
%%
\section{Реализация алгоритма C4.5}



% 2.1 Инструменты"
\subsection{Инструменты и технологии}

Наиболее распространенными инструментами для работы с алгоритмами машинного обучения на сегодняшний день являются языки R \cite{hedli_yasic} и Python \cite{koelyo_postr}. В работе используется версия Python 2.7.6. Обращаем внимание на то, что мажорные версии Python 3 и 2 не являются обратно-совместимыми. 

Python - мультипарадигмальный (объектно-ориентированный, рефлективный, императивный, функциональный и др.) высокоуровневый язык программирования, предназначенный для решения разнообразных задач, в том числе - задач машинного обучения \cite{hastie_elements}. 

В языке уже присутствует документированная библиотека Scikit-Learn \cite{jeron_priclad}, в которой реализовано большое количество алгоритмов машинного обучения, в том числе и для деревьев решений. Наша реализация алгоритма C4.5 претендует на гибкое использование проверки, усечения вершин и атрибута мульти-расщепления.


% 2.2 Реализация"
\subsection{Реализация}

На первом этапе создадим класс data, который будет использоваться при загрузке данных из файлов с расширением .csv. 

\begin{python}
class data:
    def __init__(self, datafile, test=False):
        self.datafile = datafile
        self.read_data()

    def read_data(dataset, datafile, datatypes):
    	print "Reading data..."
    	f = open(datafile)
    	original_file = f.read()
    	rowsplit_data = original_file.splitlines()
    	dataset.examples = [rows.split(',') for rows in rowsplit_data]

    	#list attributes
    	dataset.attributes = dataset.examples.pop(0)

	    #create array that indicates whether each attribute is a numerical value or not
   	 attr_type = open(datatypes) 
    	orig_file = attr_type.read()
    	dataset.attr_types = orig_file.split(',')
 \end{python}


Файлы состоят из данных и их атрибутов. Поэтому в функции read\_data() мы будем считывать из разных файлов примеры, атрибуты и типы атрибутов. 

\begin{python}
f = open('PATH_CSV') 
        original_file = f.read()
        rowsplit_data = original_file.split("\r")
        self.examples = [rows.split(',') for rows in rowsplit_data]

        self.attributes = self.examples.pop(0)
        
        attr_type = open('PATH_ATTR_CSV') 
        orig_file = attr_type.read()
        self.attr_types = orig_file.split(',')

        for example in self.examples:
            for x in range(len(self.examples[0])):
                if self.attributes[x] == 'True':
                    example[x] = float(example[x])
\end{python}

Для препроцессинга данных используется функция preprocess2() фильтрации по полю attr:

\begin{python}
def preprocess2(dataset)
\end{python}

Добавим функцию entropy() для подсчета энтропии, описание которой было дано выше. 

\begin{python}
def entropy(instances, attributes, classifier):
    count = one_count(instances, attributes, classifier)
    total = len(instances)
    count0 = total - count
    s = (-(count/total) * math.log2(count/total)) +
     (-(count0/total) * math.log2(count0/total))
    return s
\end{python}
    
    Основным классом является класс treeNode(), который будет представлять дерево решений.

\begin{python}
class treeNode():
    def __init__(self, is_leaf, classification, attr_split_index, 
    	attr_split_value, parent, upper_child, lower_child, height):
        self.is_leaf = True
        self.classification = None
        self.attr_split = None
        self.attr_split_index = None
        self.attr_split_value = None
        self.parent = parent
        self.upper_child = None
        self.lower_child = None
        self.height = None
\end{python}
    

 Для его построения используется рекурсивная функция compute\_tree():
 
\begin{python}

def compute_tree(dataset, parent_node, classifier):
    node = treeNode(True, None, None, None, 
    	parent_node, None, None, 0)
    if (parent_node == None):
        node.height = 0
    else:
        node.height = node.parent.height + 1

    ones = one_count(dataset.examples, dataset.attributes, 
    	classifier)
    if (len(dataset.examples) == ones):
        node.classification = 1
        node.is_leaf = True
        return node
    elif (ones == 0):
        node.classification = 0
        node.is_leaf = True
        return node
    else:
        node.is_leaf = False

    attr_to_split = None
    max_gain = 0
    split_val = None 
    min_gain = 0.01
    dataset_entropy = calc_dataset_entropy(dataset, classifier)

    for attr_index in range(len(dataset.examples[0])):

        if (dataset.attributes[attr_index] != classifier):
            local_max_gain = 0
            local_split_val = None
            attr_value_list = [example[attr_index]
            	 for example in dataset.examples]
            attr_value_list = list(set(attr_value_list))
            if(len(attr_value_list) > 100):
                attr_value_list = sorted(attr_value_list)
                total = len(attr_value_list)
                ten_percentile = int(total/10)
                new_list = []
                for x in range(1, 10):
                    new_list.append(attr_value_list[x*ten_percentile])
                attr_value_list = new_list

            for val in attr_value_list:
                local_gain = calc_gain(dataset, dataset_entropy, val, 
                	attr_index)
  
                if (local_gain > local_max_gain):
                    local_max_gain = local_gain
                    local_split_val = val

            if (local_max_gain > max_gain):
                max_gain = local_max_gain
                split_val = local_split_val
                attr_to_split = attr_index

    if (split_val is None or attr_to_split is None):
        print "Something went wrong. 
        Couldn't find an attribute to split on (or a split value)."
    elif (max_gain <= min_gain or node.height > 20):

        node.is_leaf = True
        node.classification = classify_leaf(dataset, classifier)

        return node

    node.attr_split_index = attr_to_split
    node.attr_split = dataset.attributes[attr_to_split]
    node.attr_split_value = split_val
#SPLIT
    upper_dataset = data(classifier)
    lower_dataset = data(classifier)
    upper_dataset.attributes = dataset.attributes
    lower_dataset.attributes = dataset.attributes
    upper_dataset.attr_types = dataset.attr_types
    lower_dataset.attr_types = dataset.attr_types
    for example in dataset.examples:
        if (attr_to_split is not None and 
        		example[attr_to_split] >= split_val):
            upper_dataset.examples.append(example)
        elif (attr_to_split is not None):
            lower_dataset.examples.append(example)

    node.upper_child = compute_tree(upper_dataset, node, classifier)
    node.lower_child = compute_tree(lower_dataset, node, classifier)

    return node
\end{python}
    
 Для классификации сета данных используется функция:
 
\begin{python}
 def classify_leaf(dataset, classifier):
    ones = one_count(dataset.examples, dataset.attributes, 
    	classifier)
    total = len(dataset.examples)
    zeroes = total - ones
    if (ones >= zeroes):
        return 1
    else:
        return 0
\end{python}

Для того, чтобы исключить влияние переобучения и утяжеления дерева, была создана функция подрезки:

\begin{python}

def prune_tree(root, node, dataset, best_score):
    if (node.is_leaf == True):
        classification = node.classification
        node.parent.is_leaf = True
        node.parent.classification = node.classification
        if (node.height < 20):
            new_score = validate_tree(root, dataset)
        else:
            new_score = 0
        if (new_score >= best_score):
            return new_score
        else:
            node.parent.is_leaf = False
            node.parent.classification = None
            return best_score
    else:
        new_score = prune_tree(root, node.upper_child, dataset, best_score)
        if (node.is_leaf == True):
            return new_score
        new_score = prune_tree(root, node.lower_child, dataset, new_score)
        if (node.is_leaf == True):
            return new_score
        return new_score
        
\end{python}

Полный код приложения представлен на открытом репозитории https://github.com/akhmanova/c4.5 и в Приложении ~\ref{pril-1}

% 2.3 Задача определения победителя "
\subsection{Задача определения победителя}

Для того, чтобы протестировать работу алгоритма были использованы данные спортивных соревнований. По 12 дискретным признакам необходимо определить вероятность выигрыша. 

\begin{lstlisting}[language=bash,caption={bash version}]
python decision-tree.py btrain.csv winner  \
	-d datatypes.csv -v bvalidate.csv -p -s
\end{lstlisting}

 Эта команда запускает алгоритм С4.5. Данные для обучения взяты из btrain.csv, данные для проверки -- bvalidate.csv. Для классификации используется параметр "winner". Флаг -p используется для подрезки, флаг -s для печати. 
 \begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{term.png}
	\caption{\label{fig:term}
	 Результаты выполнения программы}
\end{figure}

 \begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{before.png}
	\caption{\label{fig:before}
	 Результаты выполнения до усечения ветвей}
\end{figure}

 \begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{after.png}
	\caption{\label{fig:after}
	 Результаты выполнения после усечения ветвей}
\end{figure}

На рисунке ~\ref{fig:before} видно процентное соотношение верных результатов до подрезки. Точность определения победителя колеблется около $91,2 \%$.

Модификация алгоритма позволила улучшить результаты определения победителя в среднем на   $1,5 \%$. Можно заметить на рисунке  ~\ref{fig:after}, что вероятность определения победителя выросла до $92,7 \%$. 

Таким образом при 12 признаках благодаря усечению ветвей удалось удалить минимизировать эффект переобучения, упрощая слишком детализированные деревья, которые приносили  $1,5 \%$ ошибок.




% Раздел "Заключение"
\conclusion
В рамках выполнения данной курсовой работы нами были изучены деревья принятия решений, а также алгоритмы, связанные с ними (ID3, C4.5, CART, MARS) и задачи классификации и регрессии, для которых они используются.  

Особое внимание было уделено алгоритму C4.5, его математическому аппарату и реализации на языке программирования Python. Во время реализации был создан репозиторий для хранения кода в открытом доступе. Таким образом, любой пользователь сети Интернет может воспользоваться кодом для решения задач классификации. Эти задачи могут принадлежать различным предметным областям (медицина, статистика, экономика и др.) и должны быть размечены на признаки, каждый из которых должен иметь дискретное выражение. Особенностью выбранного алгоритма служит возможность содержания в данных пропущенных признаков. 

По итогам тестирования алгоритма на размеченных данных спортивных соревнований было получено дерево решений. Точность решения задачи (определение победителя в соревновании) составила $91,2 \%$ процента до усечения ветвей и $92,7 \%$ после. Этот результат можно считать достаточным для решения задачи классификации с помощью деревьев принятия решений. 

%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix



\section{Листинг программы}\label{pril-1}
Код приложения \verb"decision-tree.py".

\pythonexternal{decision-tree.py}

%[fontsize=\small, numbers=left, numbersep=2pt]{task.pl}
\end{document}
