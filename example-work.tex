\documentclass[bachelor, zaoch, coursework]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
%    pract      - отчет о научно-исследовательской работе
%    autoref    - автореферат выпускной работы
%    assignment - задание на выпускную квалификационную работу
%    review     - отзыв руководителя
%    critique   - рецензия на выпускную работу
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal
%               по умолчанию выключен
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage{graphicx}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{longtable}
\usepackage{array}


% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

\usepackage[english,russian]{babel}


\usepackage[colorlinks=true]{hyperref}


% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{математической кибернетики и компьютерных наук}

% Тема работы
\title{Деревья решений. Алгоритм C4.5}

% Курс
\course{2}

% Группа
\group{251}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
%\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{02.03.02 "--- Фундаментальная информатика и информационные технологии}
%\napravlenie{02.03.01 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{09.03.01 "--- Информатика и вычислительная техника}
\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
\studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Ахмановой Элины Дамировны}

% Заведующий кафедрой
\chtitle{к.\,ф.-м.\,н.} % степень, звание
\chname{С.\,В.\,Миронов}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент, к.\,ф.-м.\,н.} %должность, степень, звание
\saname{А.\,С.\,Иванова}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
%\patitle{к.\,ф.-м.\,н., доцент}
%\paname{Д.\,Ю.\,Петров}

% Семестр (только для практики, для остальных
% типов работ не используется)
%\term{2}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{учебная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{2}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
%%\practFinish{14.07.2016}

% Год выполнения отчета
\date{2018}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
%\secNumbering


\tableofcontents

% Раздел "Обозначения и сокращения". Может отсутствовать в работе
%\abbreviations

% Раздел "Определения". Может отсутствовать в работе
%\definitions

% Раздел "Определения, обозначения и сокращения". Может отсутствовать в работе.
% Если присутствует, то заменяет собой разделы "Обозначения и сокращения" и "Определения"
%\defabbr


% Раздел "Введение"
\intro
Выполненная курсовая работа посвящена решающим деревьям и алгоритму C4.5. Решающие деревья - это семейство моделей, позволяющих восстанавливать нелинейные зависимости разной сложности (ссылка https://ru.coursera.org/lecture/supervised-learning/rieshaiushchiie-dieriev-ia-HZxD1). Эти деревья относят к классу логических методов (ссылка J. Ross Quinlan. C4.5: Programs for Machine learning. Morgan Kaufmann Publishers 1993.). Основной идеей можно считать объединение определенного количества более простых решающих правил. Именно благодаря этому реализации можно считать интерпретируемым. 


В одной из наиболее действенных реализаций, алгоритме C4.5, для достижения максимальной обучаемости используется способ объединения нескольких вершин в одну. При этом должен использоваться критерий целесообразности объединения двух вершин.

Целью данной работы является реализация алгоритма C4.5 и изучение логического аппарата решающих деревьев. Для реализации используется язык программирования Python, так как именно он является одним из наиболее оптимальных средств решения математических задач.

Первые работы по использованию решающих деревьев берут начало с 60-х годов прошлого века. Алгоритм C4.5 разработан Джоном Квинланом (ссылка Quinlan, J. R. (1990). Learning logical definitions from relations. Machine Learning, 5:239-266.) как модификация другого алгоритма того же автора. Актуальность предоставленной работы заключается в большом количестве составных задач классификации или регрессии, для которых алгоритм может быть использован в составе композиций - решающих лесов. 


%%
%ГЛАВА 1
%%
\section{Решающие деревья}

% 1.1 "Определение"
\subsection{Определение и построение решающих деревьев}
	Работа посвящена важному классу методов машинного обучения, решающим деревьям. Эти методы созданы для решения задач классификации (когда предсказываемый результат является классом), а в специальных реализациях - и для задач регрессии (когда предсказываемый результат можно рассматривать как вещественное число: цена на дом, продолжительность пребывания пациента в больнице). 
	
Особенность решающих деревьев заключается в том, что они решают задачу получения коэффициента важности всех используемых признаков. 
	
	Область применения деревья решений в настоящее время широка, но все задачи, решаемые этим аппаратом могут быть объединены в следующие три класса (ссылка Интеллектуальные системы
Авторы: Н. Соловьев, А. Семенов, А. Цыганков, Елена Чернопрудова):
\begin{itemize}
	\item Описание данных: Деревья решений позволяют хранить информацию о данных в компактной форме, вместо них мы можем хранить дерево решений, которое содержит точное описание объектов.
	\item Классификация: Деревья решений отлично справляются с задачами классификации, т.е. отнесения объектов к одному из заранее известных классов. Целевая переменная должна иметь дискретные значения.
	\item Регрессия: Если целевая переменная имеет непрерывные значения, деревья решений позволяют установить зависимость целевой переменной от независимых(входных) переменных. Например, к этому классу относятся задачи численного прогнозирования (предсказания значений целевой переменной).
\end{itemize}
	
	Решающие деревья создавались с целью попытки формализовать способ мышления, который обычно люди используют для принятия решений. Это хорошо описывает логику принятия решений банком для выдачи кредита: 

\begin{itemize}
	\item возраст клиента;
	\item заработная плата; 
	\item рабочий стаж;
	\item наличие других кредитов и кредитная история.
\end{itemize}

Также это отлично иллюстрирует процесс работы врача, когда тот говорит с пациентом, задает ему один за другим уточняющие вопросы, может дать необходимые советы. 

Дерево решений – это способ представления правил в иерархической, последовательной структуре (ссылка https://studfiles.net/preview/6172591/page:10/) (ссылка Чубуков И.А. Data Mining. Учебный курс – [http://www.intuit.ru/department/database/datamining/].). Основа такой структуры – ответы “Да” или “Нет” в случае бинарных деревьев (алгоритм CART (ссылка Breiman, L., Friedman, J. H., Olshen, R. A.,  Stone, C. J. (1984) Classification and regression trees. Belmont, CA: Wadsworth International Group.)), или на ряд вопросов, определимых динамически в случае деревьев с произвольным числом потомков (алгоритм C4-5) (ссылка Quinlan J. R. C4.5: Programs for Machine Learning. — San Mateo: Morgan Kaufmann Publishers Inc., 1993. — 302 p. — ISBN 1-5586-0238-0.  (англ.)).

Каждой вершине дерева за исключением конечной листовой соответствует вопрос с несколькими вариантами ответа. Ответы будут являться выходящими ребрами. В зависимости от ответа осуществляется переход к одной из нижних вершин. Концевая вершина, лист, соответствует одному из классов. При положительном ответе на вопрос осуществляется переход к левой части дерева, называемой левой ветвью, при отрицательном – к правой части дерева. Таким образом, внутренний узел дерева является узлом проверки определенного условия.Для каждой конкретной задачи существуют свои типы конечного узла в количестве $n+1$.

Решающие деревья являются ациклическими графами и воспроизводят логические схемы, позволяющие получить окончательное решение о классификации объекта с помощью ответов на  иерархически организованную систему вопросов (ссылка Лекция 8 Решающие деревья Лектор Сенько Олег Валентинович Курс «Математические основы теории прогнозирования» 4-й курс, III поток Сенько Олег Валентинович () МОТП, лекция 2 1 / 15). 

Обратим внимание на то, что заданный на последнем иерархическом уровне вопрос зависит от полученного на предыдущем уровне ответа. Подобные модели свойственны для использования в:

 \begin{itemize}
	\item медицине;
	\item ботанике; 
	\item  зоологии;
	\item  социологии;
	\item экономике.
\end{itemize}

 Решающее дерево называется бинарным, если каждая внутренняя или корневая вершина поставлена в соответствие только двум выходящим рёбрам. Бинарные деревья удобно использовать в моделях машинного обучения (ссылка Лекция 8 Решающие деревья Лектор Сенько Олег Валентинович Курс «Математические основы теории прогнозирования» 4-й курс, III поток Сенько Олег Валентинович () МОТП, лекция 2 1 / 15). 


Модель, представленная в виде дерева решений, является интуитивной и упрощает понимание решаемой задачи. Результат работы алгоритмов конструирования деревьев решений, в отличие, например, от нейронных сетей, представляющих собой “черные ящики”, легко интерпретируется пользователем. Это свойство деревьев решений не только важно при отнесении к определенному классу нового объекта, но и полезно при интерпретации модели классификации в целом (ссылка Чубуков И.А. Data Mining. Учебный курс – [http://www.intuit.ru/department/database/datamining/].).

Дерево решений позволяет понять и объяснить, почему конкретный объект относится к тому или иному классу. Деревья решений дают возможность извлекать правила из базы данных на естественном языке  (ссылка Чубуков И.А. Data Mining. Учебный курс – [http://www.intuit.ru/department/database/datamining/].).
 
 
 
 \textbf{Распознавание с помощью решающих деревьев}    Предположим, что бинарное дерево T используется для распознавания объектов, описываемых набором признаков $X_1, X_2, . . , X_n$ (ссылка Лекция 8 Решающие деревья Лектор Сенько Олег Валентинович Курс «Математические основы теории прогнозирования» 4-й курс, III поток Сенько Олег Валентинович () МОТП, лекция 2 1 / 15).
     
Каждой вершине $\nu$ дерева $T$ ставится в соответствие предикат, касающийся значения одного из признаков. Непрерывному признаку $X_j$ соответствует предикат вида "$X_j \geqslant \delta^x_j$", где $j$ - некоторый пороговый параметр ~\ref{pril:7}..

Категориальному признаку $X_{j^1}$ принимающему значения из множества $M_{j^1} = { a^{j^1}_1, . . , a^{i^1}_{r(j^1)}}$ ставится в соответствие предикат вида "$X_{j^1} \in M_{j^1}^{v1}$", где $M_{j^1}^{v1}$ является элементом дихотомического разбиения $M_{j^1}^{v1}, M_{j^1}^{v2}$ множества $M_{j^1}$. Выбор одного из двух, выходящих из вершины $\nu$ ребер производится в зависимости от значения предиката.

Процесс распознавания заканчивается при достижении концевой вершины (листа). Объект относится классу согласно метке, поставленной в соответствие данному листу. 

Процесс распознавания заканчивается при достижении концевой вершины (листа). Объект относится классу согласно метке, поставленной в соответствие данному листу. 

\textbf{ Обучение решающих деревьев } Рассмотрим задачу распознавания с классами $K_1, . . , K_L$. Обучение производится по обучающей выборке $\widetilde{S_t}$ и включает в себя поиск оптимальных пороговых параметров или оптимальных дихотомических разбиений для признаков $X_1, . . , X_n $ (ссылка Методы и алгоритмы анализа данных и их моделирование в MATLAB Сирота Александр Анатольевич БХВ-Петербург, 2017 - Всего страниц: 384). 

При этом поиск производится исходя из требования снижения среднего индекса неоднородности в выборках, порождаемых искомым дихотомическим разбиением обучающей выборки  $\widetilde{S_t}$ .

Индекс неоднородности вычисляется для произвольной выборки $\widetilde{S}$, содержащей объекты из классов $K_1, . . , K_L$. При этом используется несколько видов индексов, включая: 

 \begin{itemize}
	\item энтропийный индекс неоднородности;
	\item индекс Джини (ссылка Sen, Amartya (1977), On Economic Inequality (2nd ed.), Oxford: Oxford University Press); 
	\item  индекс ошибочной классификации.
\end{itemize}

Энтропийный индекс (ссылка  Shannon, Claude E. (July 1948). «A Mathematical Theory of Communication». Bell System Technical Journal 27 (3): 419. DOI:10.1002/j.1538-7305.1948.tb01338.x.) неоднородности вычисляется по формуле:

\begin{equation}
	\gamma_e(\widetilde{S}) = - \sum{P_i \ln{P_i}},
\end{equation}

где $P_i$ - доля объектов класса $K_i$ в выборке  $\widetilde{S}$. При этом принимается, что $0 \ln{(0)} = 0$. Наибольшее значение $\gamma_e(\widetilde{S})$ принимает при равенстве долей классов. Наименьшее значение $\gamma_e(\widetilde{S})$ достигается при принадлежности всех объектов одному классу. 

Индекс Джини вычисляется по формуле

\begin{equation}
	\gamma_e(\widetilde{S}) = - \sum{P_i ^2}.
\end{equation}

Индекс ошибочной классификации вычисляется по формуле

\begin{equation}
	\gamma_e(\widetilde{S}) = 1 - \max{(P_i)}.
\end{equation}

Нетрудно понять, что индексы (2) и (3) также достигают минимального значения при принадлежности всех объектов обучающей выборке одному классу. 

Предположим, что в методе обучения используется индекс неоднородности $\gamma_*$. Для оценки эффективности разбиения обучающей выборки $\widetilde{S_t}$ на непересекающиеся подвыборки $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$ используется уменьшение среднего индекса неоднородности в $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$ по отношению к $\widetilde{S_t}$.

Данное уменьшение вычисляется по формуле:

\begin{equation}
	\Delta(\gamma_*\widetilde{S_t}) = \gamma_*(\widetilde{S_t}) - P_l\gamma_*(\widetilde{S_t^l}) - P_r\gamma_*(\widetilde{S_t^r}).
\end{equation}

где $P_l$, $P_r$ являются долями $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$ в выборке $\widetilde{S_t}$. На первом этапе обучения бинарного решающего дерева ищется оптимальный предикат соответствующий корневой вершине. С этой целью оптимальные разбиения строятся для каждого из признаков из набора $X_1, . . , X_n $.  Выбирается признак $X_{i\max}$ с максимальным значением индекса $\Delta(\gamma_*\widetilde{S_t})$. Подвыборки $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$, задаваемые оптимальным предикатом для $X_{i\max}$ оцениваются с помощью критерия остановки. 

В качестве критерия остановки может быть использован простейший критерий достижения полной однородности по одному из классов. В случае, если какая-нибудь из выборок  $\widetilde{S_t ^*}$ удовлетворяет критерию остановки, то соответствующая вершина дерева объявляется концевой и для неё вычисляется метка класса. В случае, если выборка $\widetilde{S_t ^*}$  удовлетворяет критерию остановки, то формируется новая внутренняя вершина, для которой процесс построения дерева продолжается (ссылка Лекция 8 Решающие деревья Лектор Сенько Олег Валентинович Курс «Математические основы теории прогнозирования» 4-й курс, III поток Сенько Олег Валентинович () МОТП, лекция 2 1 / 15). 
 

Однако вместо обучающей выборки $\widetilde{S_t}$ используется соответствующая вновь образованной внутренней вершине $\nu$ выборка $\widetilde{S_t}$, которая равна $\widetilde{S_t ^*}$. 

Для данной выборки производятся те же самые построения, которые на начальном этапе проводились для обучающей выборки $\widetilde{S_t}$ . 

Обучение может проводиться до тех пор, пока все вновь построенные вершины не окажутся однородными по классам. Такое дерево может быть построено всегда, когда обучающая выборка не содержит объектов с одним и тем же значениям каждого из признаков, принадлежащих разным классам. Однако абсолютная точность на обучающей выборке не всегда приводить к высокой обобщающей способности в результате эффекта переобучения. 

Одним из способов достижения более высокой обобщающей способности является использования критериев остановки, позволяющих остановит процесс построения дерева до того, как будет достигнута полная однородность концевых вершин. 

Рассмотри несколько таких критериев (ссылка Bies, Robert R.; Muldoon, Matthew F.; Pollock, Bruce G.; Manuck, Steven; Smith, Gwenn; Sale, Mark E. (2006). "A Genetic Algorithm-Based, Hybrid Machine Learning Approach to Model Selection". Journal of Pharmacokinetics and Pharmacodynamics. Netherlands: Springer: 196–221.). 
    
\begin{enumerate}
	\item Критерий остановки по минимальному допустимому числу объектов в выборках, соответствующих концевым вершинам. 
	\item Критерий остановки по минимально допустимой величине индекса $\Delta(\gamma_*\widetilde{S_t})$ . Предположим, что некоторой вершине $\nu$ соответствует выборка $\widetilde{S_{\nu}}$, для которой найдены оптимальный признак вместе с оптимальным предикатом, задающим разбиение $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . Вершина $\nu $ считается внутренней, если индекc $\Delta(\gamma_*\widetilde{S_t})$ превысил пороговое значение $\tau$ и считается концевой в противном случае. 
	\item Критерий остановки по точности на контрольной выборке. Исходная выборка данных случайным образом разбивается на обучающую выборку $\widetilde{S_t}$ и контрольную выборку $\widetilde{S_c}$ . Выборка $\widetilde{S_t}$ используется для построения бинарного решающего дерева. Предположим, что некоторой вершине $\nu$ соответствует выборка $\widetilde{S_\nu}$ , для которой найдены оптимальный признак вместе с оптимальным предикатом, задающим разбиение  $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$.
	
	На контрольной выборке $\widetilde{S_c}$ производится сравнение эффективность распознающей способности деревьев $T_\nu$ и $T_nu^{++}$ . 

Деревья $T_\nu$ и $T_nu^{++}$ включают все вершины и рёбра, построенные до построения вершины $\nu$ . В дереве $T_\nu$ вершина $\nu$ считается концевой. В дереве $T_nu^{++}$ вершина $\nu$ считается внутренней, а концевыми считаются вершины, соответствующие подвыборкам $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . Распознающая способность деревьев $T_\nu$ и $T_nu^{++}$ сравнивается на контрольной выборке $\widetilde{S_c}$ . В том, случае если распознающая способность $T_nu^{++}$ превосходит распознающую способность $T_\nu$ все дальнейшие построения исходят из того, что вершина $\nu$ является концевой. В противном случае производится исследование $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ .

	\item Статистический критерий (ссылка Р 50.1.037-2002. Рекомендации по стандартизации. Прикладная статистика: Правила проверки согласия опытного распределения с теоретическим. Часть II: Непараметрические критерии. — М.: Госстандарт РФ, 2002. Электронная версия.). Заранее фиксируется пороговый уровень значимости $(P<0.05, p<0.001$ или $p<0.001)$ . Предположим, что нам требуется оценить, является ли концевой вершина, для которой найдены оптимальный признак вместе с оптимальным предикатом, задающим разбиение $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . 
	
Исследуется статистическая достоверность различий между содержанием объектов распознаваемых классов в подвыборках $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ . Для этих целей может быть использованы известные статистический критерий: Хи-квадрат и другие критерии. 

По выборкам $ \{ \widetilde{S_{\nu}^l}, \widetilde{S_{\nu}^r} \}$ рассчитывается статистика критерия и устанавливается соответствующее p-значение. В том случае, если полученное p-значение оказывается меньше заранее фиксированного уровня значимости вершина $\nu$ считается внутренней. В противном случае вершина $\nu$ считается концевой. 
\end{enumerate}

Использование критериев ранней остановки не всегда позволяет адекватно оценить необходимую глубину дерева. Слишком ранняя остановка ветвления может привести к потере информативных предикатов, которые могут быть на самом деле найдены только при достаточно большой глубине ветвления. 

В связи с этим нередко целесообразным оказывается построение сначала полного дерева, которое затем уменьшается до оптимального с точки зрения достижения максимальной обучающей способности размера путём объединения некоторых концевых вершин. Такой процесс в литературе принято называть "pruning" ("подрезка") (ссылка Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer: 2001, pp. 269-272). 

При подрезке дерева может быть использован критерий целесообразности объединения двух вершин, основанный на сравнение на контрольной выборке точности распознавания до и после проведения "подрезки".

Ещё один способ оптимизации обобщающей способности деревьев основан на учёте при "подрезке" дерева до некоторой внутренней вершины $\nu$ одновременно увеличения точности разделения классов на обучающей выборке и увеличения сложности, которые возникают благодаря ветвлению из $\nu$. 

% 1.1 "Определение"
\subsection{Алгоритмы деревьев решений}

\begin{itemize}
    \item  ID3 (Induction of Decision Tree (ссылка R. Quinlan, "Learning efficient classification procedures", Machine Learning: an artificial intelligence approach, Michalski, Carbonell  Mitchell (eds.), Morgan Kaufmann, 1983, p. 463-482. doi:10.1007) ) - рекурсивная процедура, которая принимает на вход некоторую подвыборку обучающей выборки. Когда она вызывается впервые, на вход подается вся выборка. Если все объекты выборки лежат в одном классе, образуется и возвращается новая листовая вершина, которая содержит этот класс. Иначе ищется предикат, который максимально хорошо выделяет часть классов. По этому предикату строятся две новые вершины.
    
    Этот алгоритм является наиболее старым и лаконичным способом использования решающих  деревьев. К плюсам данного алгоритма можно отнести: простоту, интерпретируемость, гибкость задания множества предикатов, возможность использования данных с пропусками, линейная трудоемкость. 
    
    Однако недостатки вытекают из того, что это жадная стратегия, и решения о ветвлении оптимальны локально. Кроме того, данная процедура очень чувствительна к составу выборки и шумовым признакам. 
    \item  C4.5 (ссылка Quinlan, J. R. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers, 1993.) - улучшенная версия алгоритма ID3, где выбор атрибута происходит на основании нормализированного прироста информации, использует алгоритм усечения решающих деревьев и избавляет от проблемы переобучения, переусложнения структуры дерева, создавая контрольную выборку данных, которая обычно вдвое короче обучающей. Эта выборка используется для сокращения количества вершин в итоговом дереве. 
    \item CART (Classification and Regression Tree) - жадный рекурсивный алгоритм построения дерева решений, использующий стратегии усечения, обобщенный для решения задач регрессии.
    \item MARS - алгоритм, который использует расширение деревьев решений для улучшения обработки цифровых данных.
\end{itemize}

% 1.1 "Определение"
\subsection{Алгоритм C4.5}

C4.5 - алгоритм построения дерева решений, созданный Россом Куинланом (ссылка J. R. Quinlan. Improved use of continuous attributes in c4.5. Journal of Artificial Intelligence Research, 4:77-90, 1996.). как модификация более простого алгоритма ID3. Этот алгоритм используется для задач классификации, поэтому его часто упоминают как статический классификатор. 

C4.5 строит деревья решений таким же образом, как это делает ID3, используя тестовую выборку и понятие информационной энтропии (ссылка Мартин Н., Ингленд Дж. Математическая теория энтропии. — М.: Мир, 1988. — 350 с.).  

Информационная энтропия - это мера неопределенности некоторой ситуаци (ссылка Шеннон К. Работы по теории информации и кибернетике. — М.: Изд. иностр. лит., 2002.). Можно также назвать ее мерой рассеяния и в этом смысле она подобна дисперсии. Но если дисперсия является адекватной мерой рассеяния лишь для специальных распределений вероятностей случайных величин (а именно – для двухмоментных распределений, в частности, для гауссова распределения), то энтропия не зависит от типа распределения.

C4.5 моделирует дерево решений с неограниченным количеством ветвей у узла. Данный алгоритм может работать только с дискретным зависимым атрибутом и поэтому может решать только задачи классификации.

Для работы алгоритма C4.5 необходимо соблюдение следующих требований:

\begin{itemize}
    \item каждая запись набора данных должна быть ассоциирована с одним из предопределенных классов, т.е. один из атрибутов набора данных должен являться меткой класса;
    
    \item классы должны быть дискретными;
    
    \item каждый пример должен однозначно относиться к одному из классов;
    
    \item количество классов должно быть значительно меньше количества записей в исследуемом наборе данных. 
\end{itemize}

Простейшая реализация алгоритма аналогична ID3:

\begin{Verbatim}[fontsize=\small, numbers=left]
Input: database D
 Tree = {}
If D is pure OR other stopping criteria Then
	Terminate
End If
For all attribute a ? D do
	Entropy for root
End For

abest = Best attribute entropy
Tree = Create a node then test a best in the root
Dv = sud_datasets from D
For all Dv do
	Tree v= C4.5(Dv)
End for
Return Tree
\end{Verbatim}

% 1.1 "Оценка сложности"
\subsection{Оценка сложности алгоритма C4.5}

Алгоритм решающего дерева имеет сложность, линейную по длине выборки. Если множество предикатов настолько богато, что на шаге всегда находится предикат, разбивающий выборку $\widetilde{S_t}$ на непустые подмножества $\widetilde{S_t^l}$ и $\widetilde{S_t^r}$,то алгоритм строит бинарное решающее дерево, безошибочно классифицирующее выборку $\widetilde{S_t}$. 

Общая схема построения дерева принятия решений выглядит следующим образом:

\begin{enumerate}
	\item Выбирается очередной атрибут, помещается в корень.
	
	\item Для всех его значений рекурсивно строится дерево в этом потомке. 
	
	Начиная со второго шага, алгоритм может быть распараллелен.

\end{enumerate}


\begin{figure}[!ht]
	\centering
	\includegraphics[width=16cm]{alg.png}
	\caption{\label{fig:alg}
	 Зависимость времени работы алгоритма от объема обучающей выборки}
\end{figure}

На рисунке ~\ref{fig:alg} изображена скорость работы трех методов:

\begin{enumerate}

	\item Линейные вычисления.
	\item  Распараллеливание в один поток (блокирование внешних потоков).
	\item Полностью распараллеленный алгоритм на 2 потока .
	
\end{enumerate}

Из графика ~\ref{fig:alg} видно, что распараллеливание алгоритма деревьев решений имеет смысл проводить только на больших объемах данных. Для случаев, где выборка данных меньше 50 элементов распараллеливание избыточно, линейный вариант алгоритма работает быстрее.


%%
%ГЛАВА 2
%%
\section{Реализация алгоритма C4.5}



% 2.1 Инструменты"
\subsection{Инструменты и технологии}

Наиболее распространенными инструментами для работы с алгоритмами машинного обучения на сегодняшний день являются языки R и Python (ссылка Хэдли Уикем, Гарретт Гроулмунд. Язык R в задачах науки о данных: импорт, подготовка, обработка, визуализация и моделирование данных = R for Data Science: Visualize, Model, Transform, Tidy, and Import Data. — Вильямс, 2017. — 592 с.) (ссылка 
Коэльё Л. П., Ричерт В. Построение систем машинного обучения на языке Python. — Перевод с английского. — М.: ДМК Пресс, 2015. — с.). В работе используется версия Python 2.7.6. Обращаем внимание на то, что мажорные версии Python 3 и 2 не являются обратно-совместимыми. 

Python - мультипарадигмальный (объектно-ориентированный, рефлективный, императивный, функциональный и др.) высокоуровневый язык программирования, предназначенный для решения разнообразных задач, в том числе - задач машинного обучения (ссылка Hastie, T., Tibshirani R., Friedman J. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. — 2nd ed. — Springer-Verlag, 2009. — 746 p. — ISBN 978-0-387-84857-0..). 

В языке уже присутствует документированная библиотека Scikit-Learn (ссылка Орельен Жерон. Прикладное машинное обучение с помощью Scikit-Learn и TensorFlow. Концепции, инструменты и техники для создания интеллектуальных систем = Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems. — Вильямс, 2018. — 688 с.), в которой реализовано большое количество алгоритмов машинного обучения, в том числе и для деревьев решений. Наша реализация алгоритма C4.5 претендует на гибкое использование проверки, усечения вершин и атрибута мульти-расщепления.


% 2.2 Реализация"
\subsection{Реализация}

На первом этапе создадим класс data, который будет использоваться при загрузке данных из файлов с расширением .csv. 

\begin{python}
class data:
    def __init__(self, datafile, test=False):
        self.datafile = datafile
        self.read_data()

    def read_data(self):
    	...
\end{python}


Файлы состоят из данных и их атрибутов. Поэтому в функции read\_data() мы будем считывать из разных файлов примеры, атрибуты и типы атрибутов. 

\begin{python}
f = open('PATH_CSV') 
        original_file = f.read()
        rowsplit_data = original_file.split("\r")
        self.examples = [rows.split(',') for rows in rowsplit_data]

        self.attributes = self.examples.pop(0)
        
        attr_type = open('PATH_ATTR_CSV') 
        orig_file = attr_type.read()
        self.attr_types = orig_file.split(',')

        for example in self.examples:
            for x in range(len(self.examples[0])):
                if self.attributes[x] == 'True':
                    example[x] = float(example[x])
\end{python}

Для препроцессинга данных используется функция preprocess2() фильтрации по полю attr:

\begin{python}
def preprocess2(dataset)
\end{python}

Добавим функцию entropy() для подсчета энтропии, описание которой было дано выше. 

\begin{python}
def entropy(instances, attributes, classifier):
    count = one_count(instances, attributes, classifier)
    total = len(instances)
    count0 = total - count
    s = (-(count/total) * math.log2(count/total)) +
     (-(count0/total) * math.log2(count0/total))
    return s
\end{python}
    
    Основным классом является класс treeNode(), который будет представлять дерево решений.

\begin{python}
class treeNode():
    def __init__(self, is_leaf, classification, attr_split_index, 
    	attr_split_value, parent, upper_child, lower_child, height):
        self.is_leaf = True
        self.classification = None
        self.attr_split = None
        self.attr_split_index = None
        self.attr_split_value = None
        self.parent = parent
        self.upper_child = None
        self.lower_child = None
        self.height = None
\end{python}
    

 Для его построения используется рекурсивная функция compute\_tree():
 
\begin{python}

def compute_tree(dataset, parent_node, classifier):
    node = treeNode(True, None, None, None, 
    	parent_node, None, None, 0)
    if (parent_node == None):
        node.height = 0
    else:
        node.height = node.parent.height + 1

    ones = one_count(dataset.examples, dataset.attributes, 
    	classifier)
    if (len(dataset.examples) == ones):
        node.classification = 1
        node.is_leaf = True
        return node
    elif (ones == 0):
        node.classification = 0
        node.is_leaf = True
        return node
    else:
        node.is_leaf = False

    attr_to_split = None
    max_gain = 0
    split_val = None 
    min_gain = 0.01
    dataset_entropy = calc_dataset_entropy(dataset, classifier)

    for attr_index in range(len(dataset.examples[0])):

        if (dataset.attributes[attr_index] != classifier):
            local_max_gain = 0
            local_split_val = None
            attr_value_list = [example[attr_index]
            	 for example in dataset.examples]
            attr_value_list = list(set(attr_value_list))
            if(len(attr_value_list) > 100):
                attr_value_list = sorted(attr_value_list)
                total = len(attr_value_list)
                ten_percentile = int(total/10)
                new_list = []
                for x in range(1, 10):
                    new_list.append(attr_value_list[x*ten_percentile])
                attr_value_list = new_list

            for val in attr_value_list:
                local_gain = calc_gain(dataset, dataset_entropy, val, 
                	attr_index)
  
                if (local_gain > local_max_gain):
                    local_max_gain = local_gain
                    local_split_val = val

            if (local_max_gain > max_gain):
                max_gain = local_max_gain
                split_val = local_split_val
                attr_to_split = attr_index

    if (split_val is None or attr_to_split is None):
        print "Something went wrong. 
        Couldn't find an attribute to split on (or a split value)."
    elif (max_gain <= min_gain or node.height > 20):

        node.is_leaf = True
        node.classification = classify_leaf(dataset, classifier)

        return node

    node.attr_split_index = attr_to_split
    node.attr_split = dataset.attributes[attr_to_split]
    node.attr_split_value = split_val
#SPLIT
    upper_dataset = data(classifier)
    lower_dataset = data(classifier)
    upper_dataset.attributes = dataset.attributes
    lower_dataset.attributes = dataset.attributes
    upper_dataset.attr_types = dataset.attr_types
    lower_dataset.attr_types = dataset.attr_types
    for example in dataset.examples:
        if (attr_to_split is not None and 
        		example[attr_to_split] >= split_val):
            upper_dataset.examples.append(example)
        elif (attr_to_split is not None):
            lower_dataset.examples.append(example)

    node.upper_child = compute_tree(upper_dataset, node, classifier)
    node.lower_child = compute_tree(lower_dataset, node, classifier)

    return node
\end{python}
    
 Для классификации сета данных используется функция:
 
\begin{python}
 def classify_leaf(dataset, classifier):
    ones = one_count(dataset.examples, dataset.attributes, 
    	classifier)
    total = len(dataset.examples)
    zeroes = total - ones
    if (ones >= zeroes):
        return 1
    else:
        return 0
\end{python}

Для того, чтобы исключить влияние переобучения и утяжеления дерева, была создана функция подрезки:

\begin{python}

def prune_tree(root, node, dataset, best_score):
    if (node.is_leaf == True):
        classification = node.classification
        node.parent.is_leaf = True
        node.parent.classification = node.classification
        if (node.height < 20):
            new_score = validate_tree(root, dataset)
        else:
            new_score = 0
        if (new_score >= best_score):
            return new_score
        else:
            node.parent.is_leaf = False
            node.parent.classification = None
            return best_score
    else:
        new_score = prune_tree(root, node.upper_child, dataset, best_score)
        if (node.is_leaf == True):
            return new_score
        new_score = prune_tree(root, node.lower_child, dataset, new_score)
        if (node.is_leaf == True):
            return new_score
        return new_score
        
\end{python}

Полный код приложения представлен на открытом репозитории https://github.com/akhmanova/c4.5 и в Приложении ~\ref{pril-1}
%Библиографический список, составленный вручную, без использования BibTeX
%
%\begin{thebibliography}{99}
%  \bibitem{Ione} Источник 1.
%  \bibitem{Itwo} Источник 2
%\end{thebibliography}

%Библиографический список, составленный с помощью BibTeX
%
\bibliographystyle{gost780uv}
\bibliography{thesis}

% Окончание основного документа и начало приложений
% Каждая последующая секция документа будет являться приложением
\appendix



\section{Листинг программы}\label{pril-1}
Код приложения \verb"decision-tree.py".

\pythonexternal{decision-tree.py}

%[fontsize=\small, numbers=left, numbersep=2pt]{task.pl}
\end{document}
